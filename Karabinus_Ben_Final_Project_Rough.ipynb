{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Sales Opportunity Scoring</center>\n",
    "\n",
    "## <center>Submitted By</center>\n",
    "\n",
    "## <center>Ben Karabinus</center>\n",
    "\n",
    "## <center>University of Denver</center>\n",
    "\n",
    "## <center> Ritchie School of Engineering and Computer Science</center>\n",
    "\n",
    "## <center>COMP 4447, Data Science Tools 1</center>\n",
    "\n",
    "## <center>Spring Quarter 2022</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter Notebook used to conduct this analysis is hosted in the following GitHub Repositiory: __[TOOL1_FINAL_PROJECT](https://github.com/bkarabin1290/TOOL1_FINAL_PROJECT)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Opportunity Scoring Motivation_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sales opportunity can be defined as a person or sales account that has been qualified as showing legitimate interest in purchasing a product or service from another business entity. Businesses with sales departments commonly track sales opportunities through software using a customer relationship management system (CRM). CRM applications store attributes related to opportunities in an organization's sales pipeline. Attributes regarding a given opportunity may include what products or services are potentially being purchased, details of any exisitng relationship with the individual or company listed on an opportunity, financial information regarding the individual or company listed on an opportunity record, etc. Sales teams then use this information to determine which sales pursuits have the greatest liklihoood of success and therefore should be pursued more aggressively. By no means is this an exact science. Many organizations use this data to create dashboards displaying key performance metrics like sales leaders for the current fiscal year, expected revenue in the sales pipline, or win percentage. These measures are useful in understanding how a sales department is operating from a high-level, but they fail to provide a quantifiable method of predicting whether a given sales opportunity will result in a win or loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import necessary libraries\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import re\n",
    "from sklearn.preprocessing import OrdinalEncoder, RobustScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool, metrics, cv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read in the data\n",
    "\"\"\"\n",
    "opportunities = pd.read_pickle('opportunities.p')\n",
    "NAICS = pd.read_pickle('NAICS.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Data Cleaning and Feature Engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "- Column naming convention\n",
    "- NAICS categories must be joined with opportunities.\n",
    "- Record creation date should be subtracted from system close date to create a new feature representing days an opportunity is active.\n",
    "- The range of estimated revenue for opportunities varies greatly. A feature categorizing estimated revenue by quartile range should be added.\n",
    "- Remove non-informative data.\n",
    "- Handle missing data.\n",
    "- Separate categorical and continuous features for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3894 entries, 0 to 3893\n",
      "Data columns (total 66 columns):\n",
      " #   Column                                                       Non-Null Count  Dtype         \n",
      "---  ------                                                       --------------  -----         \n",
      " 0   (Do Not Modify) Opportunity                                  3894 non-null   object        \n",
      " 1   Entity ID (Account) (Account)                                3863 non-null   float64       \n",
      " 2   Address 1: City (Account) (Account)                          3884 non-null   object        \n",
      " 3   Address 1: State/Province (Account) (Account)                3880 non-null   object        \n",
      " 4   Address 1: County (Account) (Account)                        3676 non-null   object        \n",
      " 5   Address 1: Country/Region (Account) (Account)                3884 non-null   object        \n",
      " 6   Address 1: ZIP/Postal Code (Account) (Account)               3881 non-null   object        \n",
      " 7   Address 1: Latitude (Account) (Account)                      3746 non-null   float64       \n",
      " 8   Address 1: Longitude (Account) (Account)                     3746 non-null   float64       \n",
      " 9   Currency                                                     3894 non-null   object        \n",
      " 10  Est. Revenue (Base)                                          3894 non-null   float64       \n",
      " 11  Lead Source                                                  3891 non-null   object        \n",
      " 12  Type                                                         3894 non-null   object        \n",
      " 13  Probability                                                  3646 non-null   object        \n",
      " 14  Account Rating (Account) (Account)                           2176 non-null   object        \n",
      " 15  No. of Employees (Account) (Account)                         3829 non-null   float64       \n",
      " 16  Open Deals (Account) (Account)                               3894 non-null   int64         \n",
      " 17  Open Revenue (Account) (Account)                             3894 non-null   float64       \n",
      " 18  Customer Need                                                542 non-null    object        \n",
      " 19  Engagement Type                                              289 non-null    object        \n",
      " 20  Owner                                                        3894 non-null   object        \n",
      " 21  Account Mgmt 1 (Account) (Account)                           2080 non-null   object        \n",
      " 22  Account Mgmt 2 (Account) (Account)                           29 non-null     object        \n",
      " 23  Relationship Lead (Account) (Account)                        3894 non-null   object        \n",
      " 24  Annual Revenue (Account) (Account)                           3840 non-null   float64       \n",
      " 25  Business Developer 1 (Account) (Account)                     839 non-null    object        \n",
      " 26  Business Developer 2 (Account) (Account)                     94 non-null     object        \n",
      " 27  Created On                                                   3894 non-null   datetime64[ns]\n",
      " 28  Est. Close Date                                              3894 non-null   datetime64[ns]\n",
      " 29  System Close Date                                            3894 non-null   datetime64[ns]\n",
      " 30  Status                                                       3894 non-null   object        \n",
      " 31  Referral Contact                                             12 non-null     object        \n",
      " 32  Approval Status                                              0 non-null      float64       \n",
      " 33  PSE 1                                                        367 non-null    object        \n",
      " 34  PSE 2                                                        122 non-null    object        \n",
      " 35  CFY RSM Services Spend (Account) (Account)                   2998 non-null   float64       \n",
      " 36  Current Situation                                            0 non-null      float64       \n",
      " 37  EAL (Account) (Account)                                      386 non-null    object        \n",
      " 38  Internationally Active (Account) (Account)                   2324 non-null   object        \n",
      " 39  IPM Lost Reason (Account) (Account)                          545 non-null    object        \n",
      " 40  Microsoft Enterprise Agreement (Account) (Account)           2448 non-null   object        \n",
      " 41  Microsoft Reference Program (Account) (Account)              1007 non-null   object        \n",
      " 42  NAICS (Account) (Account)                                    3451 non-null   object        \n",
      " 43  Other Software (Account) (Account)                           54 non-null     object        \n",
      " 44  Preferred Method of Contact (Account) (Account)              3894 non-null   object        \n",
      " 45  Preferred Service (Account) (Account)                        0 non-null      float64       \n",
      " 46  Private Equity (Account) (Account)                           3435 non-null   object        \n",
      " 47  Referral Notes (Account) (Account)                           52 non-null     object        \n",
      " 48  Referring Firm (Account) (Account)                           0 non-null      float64       \n",
      " 49  Revenue Last Year (Account) (Account)                        0 non-null      float64       \n",
      " 50  Revenue This Year (Account) (Account)                        0 non-null      float64       \n",
      " 51  Territory (Account) (Account)                                0 non-null      float64       \n",
      " 52  VIP Client? (Account) (Account)                              3894 non-null   object        \n",
      " 53   Full Name (Internal Referral) (User)                        2 non-null      object        \n",
      " 54  Full or Part-time (Internal Referral) (User)                 2 non-null      object        \n",
      " 55  Function (Internal Referral) (User)                          2 non-null      object        \n",
      " 56  Job Title (Internal Referral) (User)                         2 non-null      object        \n",
      " 57  Office Location (Internal Referral) (User)                   2 non-null      object        \n",
      " 58  Category (Potential Customer) (Account)                      0 non-null      float64       \n",
      " 59  Client Status (Potential Customer) (Account)                 2955 non-null   object        \n",
      " 60  D&B Financial (Potential Customer) (Account)                 0 non-null      float64       \n",
      " 61  D&B Match Flag (Potential Customer) (Account)                1094 non-null   object        \n",
      " 62  D&B Ultimate Parent (Potential Customer) (Account)           0 non-null      float64       \n",
      " 63  Market Capitalization (Potential Customer) (Account)         0 non-null      float64       \n",
      " 64  Market Capitalization (Base) (Potential Customer) (Account)  0 non-null      float64       \n",
      " 65  PFY RSM Services Spend (Account) (Account)                   2998 non-null   float64       \n",
      "dtypes: datetime64[ns](3), float64(21), int64(1), object(41)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "\"\"\" print dataframe summary\"\"\"\n",
    "opportunities.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Rename columns for ease of use and readability*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"use combination of regular expressions to rename columns\"\"\"\n",
    "opportunities.columns = opportunities.columns.str.replace('\\([^)]*\\)', '', regex=True)\n",
    "opportunities.columns = opportunities.columns.str.replace('\\.', '', regex=True)\n",
    "opportunities.columns = opportunities.columns.str.replace(':', '')\n",
    "opportunities.columns = opportunities.columns.str.replace('/', '_')\n",
    "opportunities = opportunities.rename(columns=lambda x : x.strip())\n",
    "opportunities.columns = opportunities.columns.str.replace(' ','_')\n",
    "opportunities.columns = opportunities.columns.str.replace('Address_1', '')\n",
    "opportunities.columns = opportunities.columns.str.replace('^_', '', regex=True)\n",
    "opportunities = opportunities.rename(columns=lambda x : x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opportunity_id\n"
     ]
    }
   ],
   "source": [
    "\"\"\"rename system system generated GUID to opportunityID\"\"\"\n",
    "opportunities.rename(columns={opportunities.columns[0]: 'opportunity_id', 'Status' : 'Won'},\n",
    "                     inplace=True)\n",
    "print(opportunities.columns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"prepare NAICS codes for join\"\"\"\n",
    "opportunities['NAICS'] = opportunities['NAICS'].str.replace(r'\\D', '', regex=True)\n",
    "opportunities['NAICS'] = opportunities['NAICS'].str.strip()\n",
    "opportunities['NAICS'] = opportunities['NAICS'].fillna(value=0)\n",
    "opportunities['NAICS'].isnull().sum()\n",
    "opportunities['NAICS'] = opportunities['NAICS'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"merge opportunities and NAICS\"\"\"\n",
    "opportunities = pd.merge(opportunities, NAICS, how='inner', left_on = ['NAICS'],  right_on = ['mcg_naicscode'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Handle Missing and Non-informative Data*\n",
    "\n",
    "- Drop columns containing non-informative data\n",
    "- Replace missing values for categorical features with \"\\_missing_\"\n",
    "- Drop categorical features that take on single value\n",
    "- Replace missing numerical features with appropriate values or drop if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop additional columns brought in when merging opportunities with NAICS\"\"\"\n",
    "opportunities.drop(columns=['mcg_naicscode','mcg_name', 'mcg_sicdescription',\n",
    "                            'NAICS', 'mcg_description', 'mcg_subsectortext0'], \n",
    "                            axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"rename NAICS columns\"\"\"\n",
    "opportunities.rename(columns={'mcg_industry_displayname' : 'industry','mcg_industrysector_displayname' : 'industry_sector'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datetime64[ns]\n",
      "datetime64[ns]\n",
      "datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"verify data type for datetime columns\"\"\"\n",
    "print(opportunities['Created_On'].dtype)\n",
    "print(opportunities['Est_Close_Date'].dtype)\n",
    "print(opportunities['System_Close_Date'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"create new feature days active\"\"\"\n",
    "opportunities['days_active'] = opportunities['System_Close_Date'] - opportunities['Created_On']\n",
    "opportunities['days_active'] = opportunities['days_active'] / np.timedelta64(1, 'D')\n",
    "opportunities['days_active'][opportunities['days_active'] <= 0].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the new feature \"days_active\" shows the existence of 858 opportunities that were active for a negative amount of days. This is due to errors with data entry. For the purpose of this analysis all sales opportunities with negative days active will be removed from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop problem dates\"\"\"\n",
    "problem_dates = \\\n",
    "opportunities[['days_active', 'Created_On', 'System_Close_Date']][opportunities['days_active'] < 0].index\n",
    "opportunities.drop(index=problem_dates, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a new feature \"revenue_size\" as categorical representation of estimated revenue for a given opportunity\"\"\"\n",
    "#pd.set_option('display.float_format', lambda x : '%.2f' % x)\n",
    "# get revenue buckets by computing quartile range using pd.describe() built-in function\n",
    "revenue_range = opportunities['Est_Revenue'].describe()\n",
    "opportunities.loc[opportunities['Est_Revenue'].between(revenue_range['min'], revenue_range['25%'], 'both'), 'revenue_size'] = 'low'\n",
    "opportunities.loc[opportunities['Est_Revenue'].between(revenue_range['25%'], revenue_range['75%'], 'right'), 'revenue_size'] = 'med'\n",
    "opportunities.loc[opportunities['Est_Revenue'].between(revenue_range['75%'], revenue_range['max'], 'right'), 'revenue_size'] = 'high'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Won     1535\n",
       "Lost    1058\n",
       "Name: Won, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opportunities['Won'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate continuous and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "opportunity_id    0\n",
       "Entity_ID         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"ensure no missing values in key columns\"\"\"\n",
    "opportunities[['opportunity_id', 'Entity_ID']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"separate numerical, categorical, and location features\"\"\"\n",
    "opportunityID = opportunities[['opportunity_id', 'Entity_ID']].copy()\n",
    "# select numeric feature vectors\n",
    "opp_numeric = opportunities.select_dtypes(include=[np.number])\n",
    "# select discrete feature vectors\n",
    "opp_non_numeric = opportunities.select_dtypes(exclude=[np.number])\n",
    "opp_non_numeric.drop(columns=['City', 'State_Province', 'Country_Region','County',\n",
    "                             'ZIP_Postal_Code'], inplace=True)\n",
    "# select geographic feature vectors\n",
    "opp_geographic = opportunities.iloc[:, 0:9].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"reset indices to unique combination of opportunity_id and Entity_ID\"\"\"\n",
    "# join key columns to data\n",
    "opp_numeric = pd.merge(opportunityID, opp_numeric, left_index=True, right_index=True, how='inner')\n",
    "opp_non_numeric = pd.merge(opportunityID, opp_non_numeric, left_index=True, right_index=True, how='inner')\n",
    "# drop resulting duplicates and unused columns\n",
    "opp_numeric.drop(columns=['Entity_ID_y', 'Latitude', 'Longitude'], axis=1, inplace=True)\n",
    "opp_non_numeric.drop(columns=['opportunity_id_y'], axis=1, inplace=True)\n",
    "opp_numeric.rename(columns={'Entity_ID_x' : 'Entity_ID'}, inplace=True)\n",
    "opp_non_numeric.rename(columns={'opportunity_id_x' : 'opportunity_id'}, inplace=True)\n",
    "# set indices to key columns\n",
    "opp_numeric.set_index(['opportunity_id', 'Entity_ID'], inplace=True)\n",
    "opp_non_numeric.set_index(['opportunity_id', 'Entity_ID'], inplace=True)\n",
    "opp_geographic.set_index(['opportunity_id', 'Entity_ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fillna for categorical features with value \"_missing_\" \"\"\"\n",
    "opp_non_numeric.fillna(value='_missing_', inplace=True)\n",
    "opp_non_numeric.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop categorical predictors which take on a single value.\n",
    "They will contribute little to no inforamtion to the final model\n",
    "\"\"\"\n",
    "# encode discrete features as integers for use with sklearn VarianceThreshold()\n",
    "variance_threshold = VarianceThreshold(threshold=0.0)\n",
    "oe = OrdinalEncoder()\n",
    "opp_non_numeric[opp_non_numeric.columns] = oe.fit_transform(opp_non_numeric)\n",
    "variance_threshold.fit(opp_non_numeric)\n",
    "# record features which meet threshold criteria\n",
    "keep_features = [feature for feature in opp_non_numeric.columns\n",
    "                 if feature in opp_non_numeric.columns[variance_threshold.get_support()]]\n",
    "# transform discrete features to their original state and keep those which meet criteria\n",
    "opp_non_numeric[opp_non_numeric.columns] = oe.inverse_transform(opp_non_numeric)\n",
    "opp_non_numeric = opp_non_numeric[keep_features].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print null values for continuous features\"\"\"\n",
    "opp_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop continuous predictors which are missing values for 99.9% of observations.\n",
    "They will contribute litle to no inforamtion to the final model and cannot be used with\n",
    "sklearn data preparation pipeline.\n",
    "\"\"\"\n",
    "# set threshold: features must contain at least perc * num_records values != pd.nan\n",
    "perc = 0.001\n",
    "threshold = int(perc*opportunities.shape[0])\n",
    "opp_numeric.dropna(axis=1, thresh=threshold, inplace=True)\n",
    "opp_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing a count of null values for each continuous variable shows that the columns \"CFY_RSM_Services_Spend\" and \"PFY_RSM_Services_Spend\" are both missing 315 observations each. It's assumed that missing values in these columns represent clients who have not purchased consulting services from the firm in either of the two periods. As such, missing values will be replaced with 0. No_of_Employees and Annual_Revenue are missing 17 and 18 values respectively. Null values for these features will be filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"fill na values in opp_numeric\"\"\"\n",
    "opp_numeric['CFY_RSM_Services_Spend'] = opp_numeric['CFY_RSM_Services_Spend'].fillna(value=0)\n",
    "opp_numeric['PFY_RSM_Services_Spend'] = opp_numeric['PFY_RSM_Services_Spend'].fillna(value=0)\n",
    "opp_numeric[['No_of_Employees', 'Annual_Revenue']] = \\\n",
    "            opp_numeric[['No_of_Employees', 'Annual_Revenue']].fillna(value=0)\n",
    "opp_numeric.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Explore Categorical Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives\n",
    "- Examine frequency distribution of categorical features\n",
    "- visualize categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Transform \"Status\" to binary outcome\"\"\"\n",
    "opp_non_numeric['Won'] = opp_non_numeric['Won'].apply(lambda x : 1 if (x == 'Won') else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print win/loss percentage\"\"\"\n",
    "opp_non_numeric.value_counts('Won',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Win vs Loss by revenue_size and industry\"\"\"\n",
    "opp_non_numeric[['Won', 'revenue_size', 'industry']].groupby(['revenue_size', 'industry']).value_counts('Won', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_non_numeric.value_counts('Type', normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_non_numeric.value_counts('Client_Status', normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features \"Type\" and \"Client_Status\" provide similar infomation about a given opportunity. Type is a more informative and more complete feature. Client_Status will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"drop Client_Status\"\"\"\n",
    "opp_non_numeric.drop(columns=['Client_Status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opp_non_numeric.value_counts('Lead_Source')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot a count of win/loss by pursuit type\"\"\"\n",
    "# instantiate ploty express figure\n",
    "fig = px.histogram(opp_non_numeric, x='Won', color='Type', barmode='group')\n",
    "# update aesthetics\n",
    "fig.update_layout(title_text='Won/Lost Opportunities by Type', title_x=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"create a treemap to cluster wins by industry and industry_sector\"\"\"\n",
    "fig = px.treemap(opp_non_numeric, path=[px.Constant('all'), 'industry', 'industry_sector'], \n",
    "                 values = 'Won', title='Wins by Industry Sector')\n",
    "fig.update_layout(title_x=0.5, margin = dict(t=35, l=25, r=25, b=25))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "- Print Summary Statistics\n",
    "- Inspect Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Print summary statistics\"\"\"\n",
    "pd.set_option('display.float_format', lambda x : '%.2f' % x)\n",
    "opp_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot high-level view of distribution of key continuous predictors\"\"\"\n",
    "fig, ax = plt.subplots(2, 3, figsize=(20, 15))\n",
    "sns.histplot(opp_numeric, x='Est_Revenue', stat='density', kde=True, ax=ax[0,0])\n",
    "sns.histplot(opp_numeric, x='No_of_Employees', stat='density', kde=True, ax=ax[0, 1])\n",
    "sns.histplot(opp_numeric, x='Open_Revenue', stat='density', kde=True, ax=ax[0, 2])\n",
    "sns.histplot(opp_numeric, x='Annual_Revenue', stat='density', kde=True, ax=ax[1,0])\n",
    "sns.histplot(opp_numeric, x='CFY_RSM_Services_Spend', stat='density', kde=True, ax=ax[1, 1])\n",
    "sns.histplot(opp_numeric, x='PFY_RSM_Services_Spend', stat='density', kde=True, ax=ax[1, 2])\n",
    "plt.ticklabel_format(style='Plain')\n",
    "fig.suptitle('Distribution of Continuous Variables (High-Level)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Explore Relationship Between Variables*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "- Pearson's Correlation\n",
    "- Distribution Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation of numeric data\n",
    "sns.set(rc={\"figure.figsize\":(10, 6)})\n",
    "opp_numeric_corr = opp_numeric.fillna(value=0)\n",
    "correlation = sns.heatmap(opp_numeric_corr.corr(), cmap='rocket',\n",
    "                          annot=True).set_title('Pearson\\'s Correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *_Feature Selection_*\n",
    "\n",
    "- Combine data\n",
    "- Perform univariate feature selection using Mutual Information Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"recombine data\"\"\"\n",
    "# join continuous and categorical features on indices\n",
    "opportunities = pd.merge(opp_numeric, opp_non_numeric, left_index=True, right_index=True, how='inner')\n",
    "opportunities = pd.merge(opportunities, opp_geographic, left_index=True, right_index=True, how='inner')\n",
    "# move outcome \"Won\" to last column idx of dataframe\n",
    "cols = opportunities.columns.values.tolist()\n",
    "cols.insert(len(cols), cols.pop(cols.index('Won')))\n",
    "opportunities = opportunities[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preprocessing\"\"\"\n",
    "\n",
    "# drop columns that will not be included in final model\n",
    "opportunities.drop(columns=['Est_Close_Date', 'System_Close_Date', 'Created_On','Latitude', \n",
    "                            'Longitude', 'City', 'County', 'ZIP_Postal_Code' ], inplace=True)\n",
    "# susbset features for mutual infomation feature selection\n",
    "x = opportunities.iloc[:, :-1]\n",
    "y = opportunities.iloc[:, -1]\n",
    "# index continuous and categorical features\n",
    "numeric_idx = x.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_idx = x.select_dtypes(include=['object', 'bool']).columns\n",
    "# encode categorical features as integers\n",
    "oe = OrdinalEncoder()\n",
    "oe.fit(x[categorical_idx])\n",
    "# apply robust scaling to continuous features\n",
    "x[categorical_idx] = oe.transform(x[categorical_idx])\n",
    "numeric_scaler = RobustScaler()\n",
    "x[numeric_idx] = numeric_scaler.fit_transform(x[numeric_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"define Mutual Info classifier\"\"\"\n",
    "\n",
    "# get indices for categorical features\n",
    "discrete_idx =[]\n",
    "for col in categorical_idx:\n",
    "   discrete_idx.append(x.columns.get_loc(col))\n",
    "# fit mutual_info_class passing the indices of discrete columns as param\n",
    "importance = mutual_info_classif(x, y, random_state=14, discrete_features=discrete_idx)\n",
    "importance = pd.Series(importance, index=x.columns)\n",
    "importance.sort_values(ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot variable importance\"\"\"\n",
    "importance[0:26].sort_values(ascending=True).plot.barh(figsize=(20, 15))\n",
    "plt.title('Variable Importance')\n",
    "plt.xlabel('Mutual Information (Bits)')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *_Model Selection_*\n",
    "\n",
    "- Light Gradient Boosted Machine (lgbm)\n",
    "- Extreme Gradient Boosting (xgboost)\n",
    "- Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"preprocessing\"\"\"\n",
    "# subset top 20 features for xgboost\n",
    "x_onehot = opportunities[importance[0:26].index].copy()\n",
    "y_onehot = opportunities.iloc[:, -1].copy()\n",
    "# create index of continuous and categorical features\n",
    "numeric_idx = x_onehot.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_idx = x_onehot.select_dtypes(include=['object', 'bool']).columns\n",
    "# apply one hot encoding and robust scaler to xgboost data\n",
    "x_onehot[categorical_idx] = x_onehot[categorical_idx].astype('category')\n",
    "x_onehot = pd.get_dummies(x, drop_first=True)\n",
    "numeric_scaler = RobustScaler()\n",
    "x_onehot[numeric_idx] = numeric_scaler.fit_transform(x[numeric_idx])\n",
    "# subset top 20 features for catboost and lgbm\n",
    "# catboost and lgbm do not require one-hot encoding of categorical predictors\n",
    "x = opportunities[importance[0:26].index].copy()\n",
    "y = opportunities.iloc[:, -1].copy()\n",
    "x[categorical_idx] = x[categorical_idx].astype('category')\n",
    "x_categorical_idx = categorical_idx.tolist()\n",
    "x[numeric_idx] = numeric_scaler.fit_transform(x[numeric_idx])\n",
    "# lightgbm does not support existence of JSON special characters in feature names\n",
    "x = x.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "x_onehot_train, x_onehot_test, y_onehot_train, y_onehot_test = train_test_split(x_onehot, \n",
    "                                                                y_onehot, test_size=0.30,\n",
    "                                                                random_state=1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"utility function to print classification_report for models\"\"\"\n",
    "def print_report(y_test, predictions):\n",
    "    target_names = ['Lost', 'Won']\n",
    "    # sklearn classification report function\n",
    "    print(classification_report(y_test, predictions, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"utility function to plot feature importance for models\"\"\"\n",
    "\n",
    "def plot_importance(feature_names, feature_importance, model_type, importance_metric):\n",
    "\n",
    "    #Create a DataFrame of results\n",
    "    model_results = {'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    model_results = pd.DataFrame(model_results)\n",
    "    # sort results by decreasing feature importance\n",
    "    model_results.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "    # specify plot size\n",
    "    plt.figure(figsize=(15,8))\n",
    "    # create the plot\n",
    "    sns.barplot(x=model_results['feature_importance'], y=model_results['feature_names'])\n",
    "    # add labels\n",
    "    plt.title(model_type + ' Feature Importance')\n",
    "    plt.xlabel(importance_metric)\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"really light gbm\"\"\"\n",
    "# intantiate and fit the model\n",
    "lgbm_model = LGBMClassifier()\n",
    "lgbm_model.fit(x_train, y_train, categorical_feature=x_categorical_idx)\n",
    "# make prediction and print report.\n",
    "predictions = lgbm_model.predict(x_test)\n",
    "print_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"simple XGBoost\"\"\"\n",
    "# instantiate and fit the model\n",
    "xgboost_model = XGBClassifier()\n",
    "xgboost_model.fit(x_onehot_train, y_onehot_train)\n",
    "# make predictions and print report\n",
    "predictions = xgboost_model.predict(x_onehot_test)\n",
    "print_report(y_onehot_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"simple catboost\"\"\"\n",
    "# instantiate the model\n",
    "catboost_model = CatBoostClassifier(\n",
    "    custom_loss=[metrics.Accuracy()],\n",
    "    random_seed=42,\n",
    "    logging_level='Silent'\n",
    ")\n",
    "# fit the model and print report\n",
    "catboost_model.fit(\n",
    "    x_train, y_train,\n",
    "    cat_features=x_categorical_idx,\n",
    "    eval_set=(x_train, y_train),\n",
    "     #logging_level='Verbose',  # you can uncomment this for text output\n",
    "    plot=True\n",
    ")\n",
    "predictions = catboost_model.predict(x_test)\n",
    "print_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"plot feature importance\"\"\"\n",
    "plot_importance(x_test.columns, catboost_model.get_feature_importance(), \"Catboost\", \"Information Gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"CatBoost hyperparameter tuning\"\"\"\n",
    "# define repeated stratified k-fold cross validation sklearn\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=14)\n",
    "# define parameter search space as dictionary\n",
    "search_space = dict()\n",
    "search_space['learning_rate'] = np.linspace(0, .1, num=50)\n",
    "search_space['depth'] = np.linspace(1, 10, num=10)\n",
    "search_space['l2_leaf_reg'] = np.linspace(1, 5, num=5)\n",
    "search_space['one_hot_max_size'] = np.linspace(1, 20, num=20)\n",
    "# define CatBoost pool\n",
    "train_pool = Pool(x_train, y_train, cat_features=x_categorical_idx)\n",
    "test_pool = Pool(x_test, y_test, cat_features=x_categorical_idx)\n",
    "catboost_model = CatBoostClassifier(custom_loss=[metrics.Accuracy()], loss_function='CrossEntropy', cat_features=x_categorical_idx)\n",
    "# define randomized search procedure\n",
    "best_params = catboost_model.randomized_search(search_space, X = x_train, y=y_train, cv=cv,\n",
    "                                               n_iter = 20, partition_random_seed=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_params = best_params['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_catboost_model = CatBoostClassifier(**tuned_params, custom_loss=metrics.Accuracy(),\n",
    "                                          logging_level='Silent',random_seed=14,\n",
    "                                          iterations=1000, loss_function='CrossEntropy')\n",
    "tuned_catboost_model.fit(X=x_train, y=y_train, cat_features=x_categorical_idx)\n",
    "predictions = tuned_catboost_model.predict(x_test)\n",
    "print_report(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model.get_all_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(x_test.columns, tuned_catboost_model.get_feature_importance(), \"Catboost\", \"Information Gain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f25bf5c41ed9750fee6e834acff1598b309f3cea0a5eb382eb6a03fe96228341"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('TOOL1_FINAL_PROJECT-_N6x3YZ3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
